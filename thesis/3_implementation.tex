\section{Implementation}

In this section we want to present our implementation of the above described
concept. We are going to show how the software is structured
(\ref{structure}), discuss challenges in performance (\ref{performance}) and
explain how parameters were chosen (\ref{parameter}).

The algorithm was implemented in Python using the \textit{OpenCV} and
\textit{NumPy} libraries. OpenCV provided the different image processing methods
described in \ref{used} as well as other methods for reading, writing and
manipulating image files. NumPy, which is part of the SciPy package, is a
package for scientific numerical computing \cite{scipy}. Images in OpenCV are
represented as three-dimensional NumPy arrays. The first two dimensions of these
array encode the positions of pixels in the image, while the third dimension
encodes the three color values.

\subsection{Structure} \label{structure}

The main algorithm was implemented in a function \texttt{process\_image}. This
function takes a 3-dimensional array representing the image as an input and
outputs a tuple. The first element of that tuple is the actual output, a list of
rectangles found in the image. The second element is a list of images, which
represent different steps of the algorithm and are artifacts meant for debugging
and feedback. Each rectangle in the list is represented by a \textit{Dictionary}
with their respective position, dimensions and color.

Another function called \texttt{run\_pipeline} reads a list of images from a
given input directory and feeds them into \texttt{process\_image}. Each of the
debug images given are saved to disk in an output directory. The data from the
rectangles is encoded into \textit{JSON}-files.

% \subsubsection{\texttt{preprocessing(image)}}

\texttt{process\_image} first calls another function \texttt{preprocessing} that
executes the steps described in \ref{preprocessing}. The different processing
steps are also provided as an output for debugging and adjustment of the
parameters, see \ref{parameter}.

% \subsubsection{\texttt{detect\_rectangles(binary\_image, image)}}

Then the result of \texttt{preprocessing(image)} is then used as the input for
\texttt{detect\_rectangles}. This function executes the steps described in
\ref{rectangles}. The result is a list of rectangles and their colors. As
well as images that show a visual representation of the rectangles next to
the input image. This can be used to validate the result of the algorithm.

Leaving out debugging steps, the \texttt{detect\_rectangles} function consist of
to the following code:

\begin{lstlisting}
def detect_rectangles(preprocessed_img, original_img):
  (h1, v1) = detect_lines(preprocessed_img)
  (h2, v2) = reduce_lines(h1,v1)
  (h3, v3) = remove_lines_close_to_border(h2,v2)
  (h4, v4) = add_border_lines(h3,v3)
  (h5, v5) = connect_lines(h4,v4)
  (tl, bl, br, tr) = find_corners(h5,v5)
  rects = find_rectangles(tl, bl, tr))
  rects_with_color = find_colors_for_rects(rects, original_img)
  return rects_with_color
\end{lstlisting}

As an example Listing \ref{lst:findrectangles} shows the source code of
one of the functions \texttt{find\_rectangles}, which finds rectangles from a
list of top-left, bottom-left and top-right positions. The length of each of
these lists is the same and is expected to be the number of rectangles. The
program first sorts the top-right corners by their x position and the
bottom-left corners by their y position. When iterating through the top-left
corners, we know that their x position is going to be the same as the matching
bottom-left corner \texttt{bl}, but a larger y-coordinate. So we only select
these candidates from the \texttt{bottom\_left} list using an iterator
expression. Since we ordered the corners before from top to bottom, we know the
matching corner is going to be the first returned by the iterator. We therefore
only call the iterator once with \texttt{next}. The same logic is applied to
bottom-left corners. Using the coordinates of the matching top-right and
bottom-left corners, the rectangle is calculated and added to a list of
rectangles as a tuple. That list is then returned by the function.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:findrectangles,caption=Function for constructing rectangles from corners]
def find_rectangles(top_left, bottom_left, top_right):
    top_right.sort(key=lambda pos: pos[0])
    bottom_left.sort(key=lambda pos: pos[1])
    rectangles = []
    for x,y in top_left:
        x2,_ = next(tr for tr in top_right if tr[1] == y and tr[0] > x)
        _,y2 = next(bl for bl in bottom_left if bl[0] == x and bl[1] > y)
        w = x2 - x
        h = y2 - y
        rectangles.append((x,y,w,h))
    return rectangles
\end{lstlisting}
\end{minipage}

\subsection{Performance} \label{performance}

The performance of the algorithm was measured by timing the duration for each
image. The first implementation of the algorithm took about 1100 milliseconds per
iteration. By running timings on different parts of the processing, the
\texttt{detect\_lines} function was discovered as a bottle neck. It was
accountable for about 93 percent of the processing time.

This was the only place in the program were we iterated through all of the pixels
in the image using Python. All other operations were done using OpenCV or NumPy.
They also iterate through all of the pixel but using optimized C code.

To reduce the time of this step we decided to reimplement this function in a
compiled, more low-level language. We chose to use Rust using the \textit{rust-cpython}
library for bindings. Using the same algorithm implemented in Rust, the time of
this step was reduce from about 1020 to 40 milliseconds. The run time of the
program could be reduced by a factor of 13.

\subsection{Parameter Selection} \label{parameter}

By running the program on the images and looking at the debugging output images
for the different steps, the reasonable values for the parameters of the program
can be determined.

By running the program and looking at the results of the Thresholding for each
image, comparing it to the input image, the threshold value $t = 110$ was
determined to give reasonable results.

To better fine-tune the parameters, we selected a subset of input images and
always manually changed these parameters until the result was correct. The
resulting \textit{JSON} files with the correct results were moved to another
directory \textit{detected}. Then a step was added to the program that would
always compare the computed result with result in that directory if available.
When the result differed it would print out a warning.

Using this output the minimum length $l$, the maximum width $d$ and the size of
the erosion kernel $N$ were changed to maximize the number of recognized images
from that set. The resulting parameters were $l = 60$, $d = 70$ and $N =11$.
